{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T02:21:03.097773Z",
     "start_time": "2019-05-09T02:20:51.059780Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T02:21:07.387863Z",
     "start_time": "2019-05-09T02:21:03.110782Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                                     wow love place      1\n",
      "1                                         crust good      0\n",
      "2                                 tasti textur nasti      0\n",
      "3  stop late may bank holiday rick steve recommen...      1\n",
      "4                            select menu great price      1\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Passing result_type=’expand’ will expand list-like results to columns of a Dataframe\n",
    "dataset=pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t')\n",
    "#processe data\n",
    "def processing(row):\n",
    "    #只留下字母，清除部分用' '代替\n",
    "    review= re.sub('[^a-zA-z]',' ',row['Review'] ).lower().split()\n",
    "    #词根化\n",
    "    ps=PorterStemmer()\n",
    "    #去除虚词\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    return review\n",
    "\n",
    "dataset['Review']=dataset.apply(processing,axis=1)\n",
    "print(dataset.head())\n",
    "print(len(dataset.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T02:21:07.424975Z",
     "start_time": "2019-05-09T02:21:07.405894Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked  selectMarker\n",
      "1                                         crust good      0      0.393081\n",
      "2                                 tasti textur nasti      0      0.623970\n",
      "3  stop late may bank holiday rick steve recommen...      1      0.637877\n",
      "5                            get angri want damn pho      0      0.299172\n",
      "6                                honeslti tast fresh      0      0.702198\n",
      "762\n",
      "238\n"
     ]
    }
   ],
   "source": [
    "#split data into training and testing\n",
    "np.random.seed(2019)\n",
    "dataset['selectMarker']=np.random.uniform(0,1,len(dataset.index))\n",
    "# uniformly distributed,0.75 train\n",
    "trainSet=dataset[dataset['selectMarker']<0.75]\n",
    "testSet=dataset[dataset['selectMarker']>=0.75]\n",
    "print(trainSet.head())\n",
    "print(len(trainSet.index))\n",
    "print(len(testSet.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T02:21:07.465059Z",
     "start_time": "2019-05-09T02:21:07.456053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 378\n",
      "[0.49606299 0.50393701]\n"
     ]
    }
   ],
   "source": [
    "#P(y=Ci|X=x)=P(y=Ci)*P(X=x|y=Ci)\n",
    "po=len(trainSet[trainSet['Liked']==1])\n",
    "against=len(trainSet[trainSet['Liked']==0])\n",
    "total=po+against\n",
    "print(po,against)\n",
    "prior=np.array([against/total,po/total])\n",
    "print(prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T03:30:12.376852Z",
     "start_time": "2019-05-09T03:30:12.354794Z"
    }
   },
   "outputs": [],
   "source": [
    "#P(y=Ci|X=x)=P(y=Ci)*P(X=x|y=Ci)\n",
    "#P(y=C|X=x)=P(y=C)*P(x1,x2..xn|y=C)\n",
    "#1xn=1x2x2xn\n",
    "def row2vec(row,wordBag):\n",
    "    vec=[]\n",
    "    words=set()\n",
    "    for word in row.split(' '):\n",
    "        if word not in words:\n",
    "            words.add(word)\n",
    "    for word in wordBag:\n",
    "        if word not in words:\n",
    "            vec.append(0)\n",
    "        else:\n",
    "            vec.append(1)\n",
    "    return np.array(vec)\n",
    "\n",
    "def getWordBag(trainSet):\n",
    "    wordBag=set()\n",
    "    for row in trainSet['Review']:\n",
    "        words=set(row.split(' '))\n",
    "        for word in words:\n",
    "            if word not in wordBag and word!='':\n",
    "                wordBag.add(word)\n",
    "    return list(wordBag)\n",
    "\n",
    "class classifier:\n",
    "    def __init__(self,trainSet):\n",
    "#         self.conditionMatrix=self.getMatrix()\n",
    "        self.po=len(trainSet[trainSet['Liked']==1])\n",
    "        self.agianst=len(trainSet[trainSet['Liked']==0])\n",
    "        self.proWordFrequent={}\n",
    "        self.againstWordFrequent={}\n",
    "        self.wordBag=getWordBag(trainSet)\n",
    "        self.prior=[]\n",
    "        self.conditonMatrix=[]\n",
    "\n",
    "    \n",
    "    def getPrior(self):\n",
    "        po=self.po\n",
    "        against=self.agianst\n",
    "        total=po+against\n",
    "        print(po,against)\n",
    "        prior=np.array([against/total,po/total])\n",
    "        print(prior)\n",
    "        return prior\n",
    "    \n",
    "    def train(self):\n",
    "        self.prior=self.getPrior()\n",
    "        wb=self.wordBag\n",
    "        for row in trainSet[trainSet['Liked']==1]['Review']:\n",
    "            row=row.split(' ')\n",
    "            for word in row:\n",
    "                if word not in self.proWordFrequent:\n",
    "                    self.proWordFrequent[word]=1/self.po\n",
    "                else:\n",
    "                    self.proWordFrequent[word]+=1/self.po\n",
    "        for row in trainSet[trainSet['Liked']==0]['Review']:\n",
    "                row=row.split(' ')\n",
    "                #idx \n",
    "                for word in row:\n",
    "                    if word not in self.againstWordFrequent:\n",
    "                        self.againstWordFrequent[word]=1/self.agianst\n",
    "                    else:\n",
    "                        self.againstWordFrequent[word]+=1/self.agianst\n",
    "#         print(self.againstWordFrequent)\n",
    "#         print(self.proWordFrequent)\n",
    "        def dic2Vec(dic,wb):\n",
    "            result=[]\n",
    "            for word in wb:\n",
    "                if word not in dic.keys():\n",
    "                    result.append(0)\n",
    "                else:\n",
    "                    result.append(dic[word])\n",
    "            return np.array(result)\n",
    "        n1=dic2Vec(self.proWordFrequent,wb)\n",
    "        n2=dic2Vec(self.againstWordFrequent,wb)\n",
    "        self.conditonMatrix=np.matrix(np.array([n1,n2]))\n",
    "        print(self.conditonMatrix.shape)\n",
    "        #[n1,n2]\n",
    "\n",
    "    \n",
    "    def predict(self,testRow):\n",
    "        test=np.transpose(row2vec(testRow,self.wordBag))\n",
    "        #2xnxnx1\n",
    "        proProb=1\n",
    "        againstProb=1\n",
    "        result=np.dot(self.conditonMatrix,test)\n",
    "        if np.argmax(result)==0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T03:30:46.213973Z",
     "start_time": "2019-05-09T03:30:46.141442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 378\n",
      "[0.49606299 0.50393701]\n",
      "(2, 1327)\n",
      "0.7478991596638656\n"
     ]
    }
   ],
   "source": [
    "tmp= classifier(trainSet)\n",
    "tmp.train()\n",
    "cnt=0\n",
    "for i in range (len(testSet['Liked'].index)):\n",
    "    if tmp.predict(testSet['Review'].iloc[i])==testSet['Liked'].iloc[i]:\n",
    "        cnt+=1\n",
    "#     else:\n",
    "#         print(testSet['Review'].iloc[i],testSet['Liked'].iloc[i],tmp.predict(testSet['Review'].iloc[i]))\n",
    "print(cnt/len(testSet.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
